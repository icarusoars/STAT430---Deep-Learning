{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwaX6gJ1-nUY"
   },
   "source": [
    "# Overview \n",
    "\n",
    "Please see the [homework policy](https://fdl.thecoatlessprofessor.com/syllabus/#homework)\n",
    "for detailed instructions and some grading notes. Failure to follow instructions\n",
    "will result in point reductions. In particular, make sure to commit each \n",
    "exercise as you complete them. \n",
    "\n",
    "> \"Simple models and a lot of data trump more elaborate models based on less data.\"\n",
    "> \n",
    "> -- Peter Norvig\n",
    "\n",
    "## Grading\n",
    "\n",
    "The rubric CAs will use to grade this assignment is:\n",
    "\n",
    "| Task                                                   | Pts |\n",
    "|:-------------------------------------------------------|----:|\n",
    "| Denessness                                             | 30  |\n",
    "| The Art of Backprop                                    | 20  |\n",
    "| Descent from Below                                     | 10  |\n",
    "| Total                                                  | 60  |\n",
    "\n",
    "\n",
    "## Objectives \n",
    "\n",
    "The objectives behind this homework assignment are as follows:\n",
    "\n",
    "- Implement functions in Python;\n",
    "- Viewing differences in activation functions;\n",
    "- Applying gradient descent; and,\n",
    "- Constructing neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB5p6YDUKaPj"
   },
   "source": [
    "# Assignment - Homework 3\n",
    "STAT 430 - FDL, Spring 2020\n",
    "\n",
    "Due: **Friday, March 13th, 2020 at 6:00 PM**\n",
    "\n",
    "- **Author:** Skyler Shi\n",
    "- **NetID:** jingtao2\n",
    "\n",
    "### Collaborators\n",
    "\n",
    "If you worked with any other student in preparing these answers, please\n",
    "make sure to list their full names and NetIDs (e.g. `FirstName LastName (NetID)` ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Ivn9RloO0w3"
   },
   "source": [
    "## [30 points] Exercise 1 - Denseness\n",
    "\n",
    "Consider a neural network architecture given by: \n",
    "\n",
    "$$\\begin{align*}\n",
    "z_1^{(1)} &= w_{1,1}^{(1)} x_1 + w_{2,1}^{(1)} x_2 + b_{1}^{(1)} \\\\\n",
    "z_2^{(1)} &= w_{1,2}^{(1)} x_1 + w_{2,2}^{(1)} x_2 + b_{2}^{(1)} \\\\\n",
    "z_3^{(1)} &= w_{1,3}^{(1)} x_1 + w_{2,3}^{(1)} x_2 + b_{3}^{(1)} \\\\\n",
    "a_1^{(1)} &= g^{(1)}\\left({z_1^{(1)}}\\right) \\\\\n",
    "a_2^{(1)} &= g^{(1)}\\left({z_2^{(1)}}\\right) \\\\\n",
    "a_3^{(1)} &= g^{(1)}\\left({z_3^{(1)}}\\right) \\\\\n",
    "z_1^{(2)} &= w_{1,1}^{(2)} a_1^{(1)} + w_{2,1}^{(2)} a_1^{(2)} + w_{3,1}^{(2)} a_1^{(2)} +  b_{1}^{(2)} \\\\\n",
    "a_1^{(2)} &= g^{(2)}\\left({z_1^{(2)}}\\right) \\\\\n",
    "\\hat{y} &= a_1^{(2)}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $g(z)$ refers to a generic activation function.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- $z_i^{(l)}$ represents the linear combination value at neuron $i$ in layer $l$.\n",
    "- $a_i^{(l)}$ represents the activation value at neuron $i$ in layer $l$.\n",
    "- $w_{j,i}^{(l)}$ represents the weight parameter associated with neuron $j$ in layer $l-1$ to neuron $i$ in layer $l$.\n",
    "- $b_{i}^{(l)}$ represents the bias parameter associated with neuron $i$ in layer $l$.\n",
    "\n",
    "Let the network's **first layer** have weight and bias values of:\n",
    "\n",
    "$$\n",
    "W^{(1)} = \\begin{bmatrix} \n",
    "w_{1,1}^{(1)} & w_{2,1}^{(1)} \\\\\n",
    "w_{1,2}^{(1)} & w_{2,2}^{(1)} \\\\\n",
    "w_{1,3}^{(1)} & w_{2,3}^{(1)} \\\\\n",
    "\\end{bmatrix}_{3 \\times 2} = \\begin{bmatrix} \n",
    "1 & 1 \\\\\n",
    "2 & -2 \\\\\n",
    "3 & -1 \\\\\n",
    "\\end{bmatrix}_{3 \\times 2}, b^{(1)} = \\begin{bmatrix} \n",
    "b_{1}^{(1)} \n",
    "b_{2}^{(1)} \n",
    "b_{3}^{(1)} \n",
    "\\end{bmatrix}_{3 \\times 2} = \\begin{bmatrix} \n",
    "-1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}_{3 \\times 1}\n",
    "$$\n",
    "\n",
    "Let the network's **second layer** have weight and bias values of:\n",
    "\n",
    "$$\n",
    "W^{(2)} = \\begin{bmatrix} \n",
    "w_{1,1}^{(2)} & w_{2,1}^{(2)} & w_{3,1}^{(2)}\n",
    "\\end{bmatrix}_{1 \\times 3} = \\begin{bmatrix} \n",
    "2 & 3 & 1\n",
    "\\end{bmatrix}_{1 \\times 3},\\, b^{(2)} = \\begin{bmatrix} \n",
    "b_{1}^{(2)} \n",
    "\\end{bmatrix}_{1 \\times 1} = \\begin{bmatrix} \n",
    "-1\n",
    "\\end{bmatrix}_{1 \\times 1}\n",
    "$$\n",
    "\n",
    "Moreover, let the **data** $x$ be: \n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} \n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix}_{2 \\times 1} = \n",
    "\\begin{bmatrix} \n",
    "4 \\\\\n",
    "1 \n",
    "\\end{bmatrix}_{2 \\times 1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXf6VwekdJg9"
   },
   "source": [
    "**(a) (10 points)** Sketch the neural network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T83MnD2kTDqJ"
   },
   "source": [
    "![alt text](./1.a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5RjQuUoTJ8u"
   },
   "source": [
    "**(b) (10 points)** Consider the generic activation function $g(x)$ equivalent to the identity function $g(x) = x$. What would be the output of the network be?\n",
    "What does a linear activation function imply about the network's architecture?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjaFxI6eaikg"
   },
   "source": [
    "... show neuron-by-neuron computations ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear activation functions throughout the network imply that the network is just applying a linear transformation to the data. The entire network architecture can be simplied into a network with one input layer and one output layer and one set of weights and biases to learn. The learning problem essentially becomes a linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./1.b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5GSxBoSajDz"
   },
   "source": [
    "**(c) (10 points)** What would the output be if the generic activation function $g(x)$ is set to be equivalent to PReLU function with $\\alpha = 0.01$ in the **first layer**, $g^{(1)}(x) = \\mathrm{PReLU}(x, \\alpha = 0.01)$, and the ReLU function in the **second layer**, $g^{(2)}(x) = \\mathrm{ReLU}(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CWtTnzubTYn"
   },
   "source": [
    "... show neuron-by-neuron computations ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./1.c.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMqtA6xjbrOJ"
   },
   "source": [
    "## [20 points] Exercise 2 - The Art of Backprop\n",
    "\n",
    "Recall the network architecture given in **Exercise 1**. Let the generic\n",
    "activation function $g(x)$ be equivalent to the identity, $g(x) = x$, **across all layers**.\n",
    "\n",
    "**(a) [15 points]** Let the cost function for the network be squared-error given as: \n",
    "\n",
    "$$J(W) = \\left({y - \\hat y }\\right)^2$$\n",
    "\n",
    "Compute the partial derivatives with respect to:\n",
    "\n",
    "$\\frac{\\partial J(W)}{\\partial w_{1,1}^{(2)} }$, \n",
    "$\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)} }$, \n",
    "$\\frac{\\partial J(W)}{\\partial b_{3}^{(1)} }$\n",
    "\n",
    "_Hint:_ Recall **Exercise 1 b** computation for $\\hat y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5c1qcEnexIu"
   },
   "source": [
    "![alt text](./2.a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PSkFDUzJezsN"
   },
   "source": [
    "**(b) [5 points]** If $y = 42$, compute the realized value for each of the partial derivatives given in **(a)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXlgQpbAibQ6"
   },
   "source": [
    "![alt text](./2.b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYsHcfcHihIV"
   },
   "source": [
    "## [10 points] Exercise 3 - Descent from Below\n",
    "\n",
    "Recall the partial derivatives obtained in **Exercise 2**. \n",
    "\n",
    "**(a) [5 points]** Write what the parameter updates would be under **SGD with Momentum** for: \n",
    "\n",
    "$\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)} }$ and  $\\frac{\\partial J(W)}{\\partial b_{3}^{(1)} }$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmkXdIJGk7Xz"
   },
   "source": [
    "![alt text](./3.a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWo_Y51UkuAJ"
   },
   "source": [
    "**(b) [5 points]** Compute the parameter updates given $\\alpha = 0.5$, $\\rho = 0.1$, and $v_0 = 4$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c506otwk-5j"
   },
   "source": [
    "![alt text](./3.b.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "draft-hw03-assign.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
